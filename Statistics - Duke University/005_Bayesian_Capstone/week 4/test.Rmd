---
title: "Capstone Quiz III"
output: statsr:::statswithr_lab
---

Load libraries

```{r}
library('dplyr')
library('BAS')

```





In a simple linear regression model y~i~ = a + b*X~i~ + e~i~, y~i~ is the i~th~ observation of the dependent variable and X~i~ is the
value of the independent variable at which Y~i~ is observed. The quantities a and b are unknown parameters that represent the
intercept and slope of the regression line, respectively. The random error associated with Y~i~ is termed e~i~.

After fitting the model, each individual feature X~i~ has gotten a weight (Coefficient), which is calculated `X~i~ * weight` and adds up to the total amount of y~i~.
The weights can be retrieved via summary function (siehe above).
</br></br>
Model: `log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr`
</b></br>
The assumptions regarding the errors are, that in a population of N values of y~i~, the random errors (e~i~) have zero mean, a common variance (o^2^), and
are independent of one another.

hat{y} is obtained at each X~i~ with the fitted regression model. The quantity y~i~ - hat{y}~i~, represents the difference between the" observed
value (y) and the predicted value (y) at X~i~, and this difference is called the residual corresponding to the it observation. The larger
the values of the residuals, the less confident one feels about how well the estimated equation fits the observed values.

Based on the description above we see that the given data (each individual feature) for house number 428 in most areas is quite distant from the fitted regression line, 
and therefore adds up to the high residuals.


First, let us load the data:

```{r load}
load("Data/ames_train.Rdata")
data <- ames_train
```

```{r }
data.model <- data %>% dplyr::select( Lot.Area, Land.Slope, Year.Built, Year.Remod.Add, Bedroom.AbvGr, price)

# make sure no NA values are in the dataset
data.model <- data.model[complete.cases(data.model), ]
```


```{r Q1}
# type your code for Question 1 here, and Knit
# Bayesian Information Criterion
houses.BIC = bas.lm(formula = log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr,
                    prior = "BIC",
                    modelprior = uniform(),
                    data.model
)
image.bas(houses.BIC, subset=-1, rotate = FALSE)
```

```{r}
# Zellner-Siow Cauchy
houses.ZS =  bas.lm(formula = log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr, 
                    prior="ZS-null",
                    modelprior=uniform(),
                    method = "MCMC", 
                    MCMC.iterations = 10^6
                    data.model) 
image.bas(houses.ZS, subset=-1, rotate = FALSE)

```

