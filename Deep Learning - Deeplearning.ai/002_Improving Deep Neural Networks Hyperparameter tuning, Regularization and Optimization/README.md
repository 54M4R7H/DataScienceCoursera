## Deep Learning - Coursera
**Specialization** provided by DeepLearning.ai

### Course 2 - Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

- Week 1 - Practical aspects of Deep Learning
  - Train / Dev / Test sets
  - Bias / Variance
  - Basic Recipe for Machine Learning
  - Regularization
  - Why regularization reduces overfitting?
  - Dropout Regularization
  - Understanding Dropout
  - Other regularization methods
  - Normalizing inputs
  - Vanishing / Exploding gradients
  - Weight Initialization for Deep Networks
  - Numerical approximation of gradients
  - Gradient checking
  - Gradient Checking Implementation Notes
  - Programming Assignment: [Initialization](https://github.com/bhunkeler/DataScienceCoursera/blob/master/Deep%20Learning%20-%20Deeplearning.ai/002_Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/week%201/Programming%20Assignment/Initialization.ipynb)
  - Programming Assignment: [Regularization](https://github.com/bhunkeler/DataScienceCoursera/blob/master/Deep%20Learning%20-%20Deeplearning.ai/002_Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/week%201/Programming%20Assignment/Regularization.ipynb)
  - Programming Assignment: [Gradient Checking](https://github.com/bhunkeler/DataScienceCoursera/blob/master/Deep%20Learning%20-%20Deeplearning.ai/002_Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/week%201/Programming%20Assignment/Gradient_Checking_v1.ipynb)

- Week 2 - Optimization algorithms
  - Mini-batch gradient descent
  - Understanding mini-batch gradient descent
  - Exponentially weighted averages
  - Understanding exponentially weighted averages
  - Bias correction in exponentially weighted averages
  - Gradient descent with momentum
  - RMSprop
  - Adam optimization algorithm
  - Learning rate decay
  - The problem of local optima
  - Programming Assignment: [Optimization Methods](https://github.com/bhunkeler/DataScienceCoursera/blob/master/Deep%20Learning%20-%20Deeplearning.ai/002_Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/week%202/Programming%20Assignment/Optimization_methods.ipynb)
  
- Week 3 - Hyperparameter tuning, Batch Normalization and Programming Frameworks
  - Tuning process
  - Using an appropriate scale to pick hyperparameters
  - Hyperparameters tuning in practice: Pandas vs. Caviar
  - Normalizing activations in a network
  - Fitting Batch Norm into a neural network
  - Why does Batch Norm work?
  - Batch Norm at test time
  - Softmax Regression
  - Training a softmax classifier
  - Deep learning frameworks
  - TensorFlow
  - Programming Assignment: [Tensorflow Tutorial](https://github.com/bhunkeler/DataScienceCoursera/blob/master/Deep%20Learning%20-%20Deeplearning.ai/002_Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/week%203/Programming%20Assignment/Tensorflow_Tutorial.ipynb)
